# Awesome Vision-and-Language Navigation

A curated list of research papers in Vision-and-Language Navigation (VLN). Link to the code and website if available is also present.

## Contributing

Please feel free to contact me via email (liudq@mail.ustc.edu.cn) or open an issue or submit a pull request.

To add a new paper via pull request:
1. Fork the repo, edit `README.md`.
1. Put the new paper at the correct chronological position as the following format: <br>
    ```
    1. **Paper Title** <br>
    *Author(s)* <br>
    Conference, Year. [[Paper]](link) [[Code]](link) [[Website]](link)
    ```
1. Send a pull request. Ideally, I will review the request within a week.

## Papers

1. **Mapping Instructions and Visual Observations to Actions with Reinforcement Learning** <br>
*Dipendra Misra, John Langford, Yoav Artzi* <br>
EMNLP, 2017. [[Paper]](https://www.aclweb.org/anthology/D17-1106.pdf) [[Code]](https://github.com/lil-lab/blocks)

1. **Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction** <br>
*Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, Yoav Artzi*
EMNLP, 2018. [[Paper]](https://arxiv.org/pdf/1809.00786.pdf) [[Code]](https://github.com/lil-lab/ciff)

1. **Vision-and-Language Navigation: Interpreting Visually-Grounded Navigation Instructions in Real Environments** <br>
*Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sünderhauf, Ian Reid, Stephen Gould, Anton van den Hengel* <br>
CVPR, 2018. [[Paper]](https://arxiv.org/abs/1711.07280) [[Code]](https://github.com/peteanderson80/Matterport3DSimulator) [[Website]](https://bringmeaspoon.org)

1. **Look Before You Leap: Bridging Model-Free and Model-Based Reinforcement Learning for Planned-Ahead Vision-and-Language Navigation** <br>
*Xin Wang, Wenhan Xiong, Hongmin Wang, William Yang Wang* <br>
ECCV, 2018. [[Paper]](https://arxiv.org/abs/1803.07729)

1. **Speaker-Follower Models for Vision-and-Language Navigation** <br>
*Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor Berg-Kirkpatrick, Kate Saenko, Dan Klein, Trevor Darrell* <br>
NeurIPS, 2018. [[Paper]](https://arxiv.org/abs/1806.02724) [[Code]](https://github.com/ronghanghu/speaker_follower) [[Website]](http://ronghanghu.com/speaker_follower/)

1. **Shifting the Baseline: Single Modality Performance on Visual Navigation & QA** <br>
*Jesse Thomason, Daniel Gordon, Yonatan Bisk* <br>
NAACL, 2019. [[Paper]](https://arxiv.org/abs/1811.00613) [[Poster]](https://jessethomason.com/personal_site/www/publication_supplements/NAACL19_poster.pdf)

1. **Reinforced Cross-Modal Matching and Self-Supervised Imitation Learning for Vision-Language Navigation** <br>
*Xin Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan-Fang Wang, William Yang Wang, Lei Zhang* <br>
CVPR, 2019. [[Paper]](https://arxiv.org/abs/1811.10092)

1. **Self-Monitoring Navigation Agent via Auxiliary Progress Estimation** <br>
*Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, Caiming Xiong* <br>
ICLR, 2019. [[Paper]](https://arxiv.org/abs/1901.03035) [[Code]](https://github.com/chihyaoma/selfmonitoring-agent) [[Website]](https://chihyaoma.github.io/project/2018/09/27/selfmonitoring.html)

1. **The Regretful Agent: Heuristic-Aided Navigation through Progress Estimation** <br>
*Chih-Yao Ma, Zuxuan Wu, Ghassan AlRegib, Caiming Xiong, Zsolt Kira* <br>
CVPR, 2019. [[Paper]](https://arxiv.org/abs/1903.01602) [[Code]](https://github.com/chihyaoma/regretful-agent) [[Website]](https://chihyaoma.github.io/project/2019/02/25/regretful.html)

1. **Tactical Rewind: Self-Correction via Backtracking in Vision-and-Language Navigation** <br>
*Liyiming Ke, Xiujun Li, Yonatan Bisk, Ari Holtzman, Zhe Gan, Jingjing Liu, Jianfeng Gao, Yejin Choi, Siddhartha Srinivasa* <br>
CVPR, 2019. [[Paper]](http://arxiv.org/abs/1903.02547) [[Code]](https://github.com/Kelym/FAST) [[Video]](https://www.youtube.com/watch?v=AD9TNohXoPA)

1. **Learning to Navigate Unseen Environments: Back Translation with Environmental Dropout** <br>
*Hao Tan, Licheng Yu, Mohit Bansal* <br>
NAACL, 2019. [[Paper]](https://arxiv.org/abs/1904.04195) [[Code]](https://github.com/airsplay/R2R-EnvDrop)

1. **Multi-modal Discriminative Model for Vision-and-Language Navigation** <br>
*Haoshuo Huang, Vihan Jain, Harsh Mehta, Jason Baldridge, Eugene Ie* <br>
NAACL Workshop, 2019. [[Paper]](https://arxiv.org/abs/1905.13358)

1. **Are You Looking? Grounding to Multiple Modalities in Vision-and-Language Navigation** <br>
*Ronghang Hu, Daniel Fried, Anna Rohrbach, Dan Klein, Trevor Darrell, Kate Saenko* <br>
ACL, 2019. [[Paper]](https://arxiv.org/abs/1906.00347)

1. **Chasing Ghosts: Instruction Following as Bayesian State Tracking** <br>
*Peter Anderson, Ayush Shrivastava, Devi Parikh, Dhruv Batra, Stefan Lee* <br>
NeurIPS, 2019. [[Paper]](https://arxiv.org/abs/1907.02022)

1. **Embodied Vision-and-Language Navigation with Dynamic Convolutional Filters** <br>
*Federico Landi, Lorenzo Baraldi, Massimiliano Corsini, Rita Cucchiara* <br>
BMVC, 2019. [[Paper]](https://arxiv.org/abs/1907.02985) [[Code]](https://github.com/aimagelab/DynamicConv-agent)

1. **Transferable Representation Learning in Vision-and-Language Navigation** <br>
*Haoshuo Huang, Vihan Jain, Harsh Mehta, Alexander Ku, Gabriel Magalhaes, Jason Baldridge, Eugene Ie* <br>
ICCV, 2019. [[Paper]](https://arxiv.org/abs/1908.03409)

1. **Robust Navigation with Language Pretraining and Stochastic Sampling** <br>
*Xiujun Li, Chunyuan Li, Qiaolin Xia, Yonatan Bisk, Asli Celikyilmaz, Jianfeng Gao, Noah Smith, Yejin Choi* <br>
EMNLP, 2019. [[Paper]](https://arxiv.org/abs/1909.02244) ~~[[Code]](https://github.com/xjli/r2r_vln)~~

1. **Vision-Language Navigation with Self-Supervised Auxiliary Reasoning Tasks** <br>
*Fengda Zhu, Yi Zhu, Xiaojun Chang, Xiaodan Liang* <br>
arXiv:1911.07883. [[Paper]](https://arxiv.org/abs/1911.07883)

1. **Counterfactual Vision-and-Language Navigation via Adversarial Path Sampling** <br>
*Tsu-Jui Fu, Xin Wang, Matthew Peterson, Scott Grafton, Miguel Eckstein, William Yang Wang* <br>
arXiv:1911.07308. [[Paper]](https://arxiv.org/abs/1911.07308)
