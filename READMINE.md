# Human-Robot Interaction

A curated list of research papers in Human-Robot Interaction. Link to the code and website if available is also present.

## Papers

<!--_1. **** <br>
** <br>
RSS, 2016. [[Paper]]() [[Code]]() [[Video]]()-->

1. **Understanding natural language commands for robotic navigation and mobile manipulation** <br>
*Stefanie Tellex, Thomas Kollar, Steven Dickerson, Matthew R Walter, Ashis Gopal Banerjee, Seth Teller, Nicholas Roy* <br>
AAAI, 2011. [[Paper]](https://www.aaai.org/ocs/index.php/AAAI/AAAI11/paper/download/3623/4113)

1. **Efficient Grounding of Abstract Spatial Concepts for Natural Language Interaction with Robot Manipulators** <br>
*Rohan Paul, Jacob Arkin, Nicholas Roy, Thomas M. Howard* <br>
RSS, 2016. [[Paper]](https://core.ac.uk/download/pdf/159108268.pdf) [[Video]](https://www.youtube.com/watch?v=niDFJOVpuzc)

1. *Grounding Abstract Spatial Concepts for Language Interaction with Robots*** <br>
*Rohan Paul1, Jacob Arkin, Nicholas Roy, Thomas Howard* <br>
IJCAI, 2017. [[Paper]](https://www.ijcai.org/Proceedings/2017/0696.pdf)

1. **Mapping Instructions and Visual Observations to Actions with Reinforcement Learning** <br>
*Dipendra Misra, John Langford, Yoav Artzi* <br>
EMNLP, 2017. [[Paper]](https://www.aclweb.org/anthology/D17-1106.pdf) [[Code]](https://github.com/lil-lab/blocks)

1. **A Corpus of Natural Language for Visual Reasoning** <br>
*Alane Suhr, Mike Lewis, James Yeh, Yoav Artzi* <br>
ACL, 2017. [[Paper]](https://www.aclweb.org/anthology/P17-2034/) [[Code]](http://lil.nlp.cornell.edu/nlvr/index.html)

1. **Interactive Visual Grounding of Referring Expressions for Human-Robot Interaction** <br>
*Mohit Shridhar, David Hsu* <br>
RSS, 2018. [[Paper]](https://arxiv.org/abs/1806.03831) [[Video]](https://drive.google.com/file/d/15AttCp-KCDEt8Ys5TfqXowsElm9GqAkH/view) 

1. **Learning Interpretable Spatial Operations in a Rich 3D Blocks World** <br>
*Yonatan Bisk, Kevin Shih, Yejin Choi, and Daniel Marcu* <br>
AAAI, 2018. [[Paper]](https://yonatanbisk.com/papers/2018-AAAI.pdf) [[Code]](https://github.com/ybisk/GroundedLanguage)

1. **Mapping Instructions to Actions in 3D Environments with Visual Goal Prediction** <br>
*Dipendra Misra, Andrew Bennett, Valts Blukis, Eyvind Niklasson, Max Shatkhin, Yoav Artzi* <br>
EMNLP, 2018. [[Paper]](https://arxiv.org/pdf/1809.00786.pdf) [[Code]](https://github.com/lil-lab/ciff)

1. **Mapping navigation instructions to continuous control actions with position-visitation predictio** <br>
*Valts Blukis, Dipendra Misra, Ross A. Knepper, Yoav Artzi* <br>
CoRL, 2018. [[Paper]](https://arxiv.org/abs/1811.04179)

1. **Grounding Robot Plans from Natural Language Instructions with Incomplete World Knowledge** <br>
*Daniel Nyga, Subhro Roy, Rohan Paul, Daehyung Park, Mihai Pomarlan,
Michael Beetz, Nicholas Roy* <br>
CoRL, 2018. [[Paper]](http://proceedings.mlr.press/v87/nyga18a/nyga18a.pdf)

1. **Improving Grounded Natural Language Understanding through Human-Robot Dialog** <br>
*Jesse Thomason, Aishwarya Padmakumar, Jivko Sinapov, Nick Walker, Yuqian Jiang, Harel Yedidsion, Justin Hart, Peter Stone, Raymond J. Mooney* <br>
ICRA, 2019. [[Paper]](https://arxiv.org/abs/1903.00122) [[Video]](https://www.youtube.com/watch?v=PbOfteZ_CJc&feature=youtu.be&t=5)

1. **Learning to Generate Unambiguous Spatial Referring Expressions for Real-World Environments** <br>
*Fethiye Irmak DoÄŸan, Sinan Kalkan, Iolanda Leite* <br>
IROS, 2019. [[Paper]](https://arxiv.org/abs/1904.07165) [[Video]](https://www.youtube.com/watch?v=BE6-F6chW0w&feature=youtu.be)

1. **Expressing Visual Relationships via Language** <br>
*Hao Tan, Franck Dernoncourt, Zhe Lin, Trung Bui, and Mohit Bansal* <br>
ACL, 2019. [[Paper]](https://arxiv.org/abs/1906.07689) [[Code]](https://github.com/airsplay/VisualRelationships)

1. **A Corpus for Reasoning About Natural Language Grounded in Photographs** <br>
*Alane Suhr, Stephanie Zhou, Ally Zhang, Iris Zhang, Huajun Bai, Yoav Artzi* <br>
ACL, 2019. [[Paper]](https://arxiv.org/abs/1811.00491) [[Code]](http://lil.nlp.cornell.edu/nlvr/)

1. **Learning to Map Natural Language Instructions to Physical Quadcopter Control using Simulated Flight** <br>
*Valts Blukis, Yannick Terme, Eyvind Niklasson, Ross A. Knepper, Yoav Artzi* <br>
CoRL, 2019. [[Paper]](https://arxiv.org/abs/1910.09664) [[Code]](https://github.com/lil-lab/drif) [[Video]](https://www.youtube.com/watch?v=O7G0HYGqU4w)

1. **LXMERT: Learning Cross-Modality Encoder Representations from Transformers** <br>
*Hao Tan, Mohit Bansal* <br>
EMNLP, 2019. [[Paper]](https://arxiv.org/abs/1908.07490) [[Code]](https://github.com/airsplay/lxmert)

1. **Enabling Robots to Understand Incomplete Natural Language Instructions Using Commonsense Reasoning** <br>
*Haonan Chen, Hao Tan, Alan Kuntz, Mohit Bansal, and Ron Alterovitz* <br>
ICRA, 2020. [[Paper]](https://arxiv.org/abs/1904.12907) [[Video]](https://www.youtube.com/watch?v=W5wYFd7aJP0)

1. **Modality-Balanced Models for Visual Dialogue** <br>
*Hyounghun Kim, Hao Tan, and Mohit Bansal* <br>
AAAI, 2020. [[Paper]](https://arxiv.org/abs/2001.06354)

1. **REVERIE: Remote Embodied Visual Referring Expression in Real Indoor Environments** <br>
*Yuankai Qi, Qi Wu, Peter Anderson, Xin Wang, William Yang Wang, Chunhua Shen, Anton van den Hengel* <br>
CVPR, 2020. [[Paper]](https://arxiv.org/abs/1904.10151) [[Code]](https://github.com/YuankaiQi/REVERIE) 

1. **ALFRED: A Benchmark for Interpreting Grounded Instructions for Everyday Tasks** <br>
*Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, Dieter Fox* <br>
CVPR, 2020. [[Paper]](https://arxiv.org/abs/1912.01734) [[Code]](https://github.com/askforalfred/alfred) 

1. **Diagnosing the Environment Bias in Vision-and-Language Navigation** <br>
*Yubo Zhang, Hao Tan, Mohit Bansal* <br>
IJCAI, 2020. [[Paper]](https://arxiv.org/abs/2005.03086) [[Code]](https://github.com/zhangybzbo/EnvBiasVLN)

1. **Experience Grounds Language** <br>
*Yonatan Bisk, Ari Holtzman, Jesse Thomason, Jacob Andreas, Yoshua Bengio, Joyce Chai, Mirella Lapata, Angeliki Lazaridou, Jonathan May, Aleksandr Nisnevich, Nicolas Pinto, Joseph Turian* <br>
arXiv, 2020. [[Paper]](https://arxiv.org/abs/2004.10151)

1. **RMM: A Recursive Mental Model for Dialog Navigation** <br>
*Homero Roman Roman, Yonatan Bisk, Jesse Thomason, Asli Celikyilmaz, Jianfeng Gao* <br>
arXiv, 2020. [[Paper]](https://arxiv.org/abs/2005.00728)
